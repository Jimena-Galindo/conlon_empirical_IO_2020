{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical IO PhD Class\n",
    "## Problem Set 0\n",
    "Jimena, Eyal and Pietro \n",
    "\n",
    "September 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Logit Function\n",
    "** 1. The log-sum-exp function is convex everywhere: **\n",
    "\n",
    "Pick any distinct $x, y \\in \\mathbb{R}^{N+1}$ and any $\\alpha \\in (0,1)$. \n",
    "$$f ( \\alpha x + ( 1 - \\alpha ) y ) = \\log \\sum_{i=0}^N \\exp(\\alpha x_i + ( 1 - \\alpha ) y_i)$$\n",
    "Applying Hölder's inequality to $\\sum_{i=0}^N \\exp(\\alpha x_i) \\exp( ( 1 - \\alpha ) y_i)$ with exponents $\\frac{1}{\\alpha}$ and $\\frac{1}{1-\\alpha}$ we get \n",
    "$$ \\sum_{i=0}^N \\exp(\\alpha x_i) \\exp( ( 1 - \\alpha ) y_i ) \\leq \\left [ \\sum_{i=0}^N |\\exp(\\alpha x_i)|^{\\frac{1}{\\alpha}}\\right ]^\\alpha \\left [ \\sum_{i=0}^N |\\exp((1 - \\alpha) y_i)|^{\\frac{1}{1 - \\alpha}} \\right ]^{1 - \\alpha}$$\n",
    "Taking logs on both sides and rearranging: \n",
    "$$ \\log \\sum_{i=0}^N exp( \\alpha x_i + ( 1 - \\alpha ) y_i) \\leq \\alpha \\log \\sum_{i=0}^N exp( x_i ) + ( 1 - \\alpha )\\log \\sum_{i=0}^N exp( y_i )  $$\n",
    "So the function is convex everywhere. \n",
    "\n",
    "** 2. Using the max trick: **\n",
    "\n",
    "Fix some $x \\in \\mathbb{R}^{N+1}$ and let $m:=\\max_i x_i$. Assume wlog $x_0=m$. \n",
    "We have \n",
    "$$ IV = \\log \\sum_{i=0}^N exp(x_i) = \\log \\sum_{i=0}^N exp(x_i) \\frac{exp(m)}{exp(m)}  $$\n",
    "and rearranging \n",
    "$$ IV = m + \\log ( 1 + \\sum_{i=1}^N \\exp ( x_i-m ) ) $$\n",
    "We have rescaled everything relative to the $\\max$ and added a constant. With this we take the exponential of smaller numbers and avoid the overflow problem. \n",
    "\n",
    "** 3. Comparing it to scipy.misc.logsumexp. Does it appear to suffer from underflow/overflow?\n",
    "Does it use the max trick?**\n",
    "\n",
    "First generate a tuple of values which includes some $x_i>600$ and evaluate the function at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "x=np.arange(10, 800, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the original IV equation we get the error (the evaluated number is infinity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pietro\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_1=np.log(np.sum(np.exp(x)))\n",
    "IV_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the *max* trick and compare to the value that we get from the logsumexp function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790.69316988129776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=max(x)\n",
    "IV=m+np.log(1+np.sum(np.exp(x-m)))\n",
    "IV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790.0000454009604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logsumexp and the function we modified have similar results but not exactly the same so logsumexp is doing something else to compute IV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### Stationary Distribution from eigenvectors\n",
    "Write a function that computes the ergodic distribution of the matrix\n",
    "$P$ by examining the properly rescaled eigenvectors and compare your result to $P^{100}$.\n",
    "\n",
    "We first define the transition matrix P and take the 100th power of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the transition matrix P \n",
      " [[ 0.2  0.4  0.4]\n",
      " [ 0.1  0.3  0.6]\n",
      " [ 0.5  0.1  0.4]]\n",
      "and this is the 100th power \n",
      " [[ 0.31034483  0.24137931  0.44827586]\n",
      " [ 0.31034483  0.24137931  0.44827586]\n",
      " [ 0.31034483  0.24137931  0.44827586]]\n",
      "\n",
      " this is the stationary using Quantecon package [[ 0.31034483  0.24137931  0.44827586]]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0.2, 0.4, 0.4],[0.1, 0.3, 0.6],[0.5, 0.1, 0.4]])\n",
    "print(\"This is the transition matrix P \\n\", P)\n",
    "P_100 = np.linalg.matrix_power(P, 100)\n",
    "print(\"and this is the 100th power \\n\", P_100)\n",
    "\n",
    "import quantecon as qe\n",
    "stat= qe.markov.core.mc_compute_stationary(P)\n",
    "print(\"\\n this is the stationary using Quantecon package\", stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iteration approach and the almighty John Stachurski agree on the stationary distribution. Let us try to compute it using the eigenvector method too.\n",
    "To calculate the stationary distribution we compute the eigenvalues and left eigenvectors of P. One of the eigenvalues is equal to one as P is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the eigenvalues are \n",
      " [ 1.00+0.j         -0.05+0.23979158j -0.05-0.23979158j]\n"
     ]
    }
   ],
   "source": [
    "P_T = np.matrix.transpose(P)\n",
    "w,v = np.linalg.eig(P_T)\n",
    "print(\"the eigenvalues are \\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first eigenvalue is equal to one, so the eigenvector associated to it will be our stationary distribution. Notice that the distribution needs to be a row vector so we need to transpose the eigenvectors we found. Indeed the stationary distribution $\\Pi$ satisfies $\\Pi' = P'\\Pi'$. I.e. it is the transpose of the right eigenvector associated with the unit eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvector we consider is \n",
      " [-0.52048344 -0.40482045 -0.75180941]\n"
     ]
    }
   ],
   "source": [
    "eigenvector = np.real( v.transpose()[0])\n",
    "print(\"The eigenvector we consider is \\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to rescale it so that the elements sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ergodic distribution found using eigenvectors is \n",
      " [ 0.31034483  0.24137931  0.44827586]\n"
     ]
    }
   ],
   "source": [
    "normalization = np.sum(eigenvector)\n",
    "Π = eigenvector / normalization \n",
    "print(\"The ergodic distribution found using eigenvectors is \\n\",Π)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare with what we found using matrix power, we find the two values are essentially identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of absolute discrepancies between the two matrices is \n",
      " 1.26842980563e-14\n"
     ]
    }
   ],
   "source": [
    "diff = np.sum ( np.abs(Π -P_100) )\n",
    "print(\"The sum of absolute discrepancies between the two matrices is \\n\",diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationaty Distribution from system of equations\n",
    "Just to show off our mathematical prowess, this section solves for the stationary distribution in a different way, solving the system of equations that define it.\n",
    "\n",
    "We want to have $\\pi$ such that $ (P^T-I)\\pi=0 $ and $\\sum_i \\pi_i=1$. We can write this as a system of equations $A\\pi=b$ with $A^T=[P^T-I, \\mathbb{1}]$ and $b=[\\mathbb{0}, 1]^T$ so that we can solve for $\\pi$ in $A^T A \\pi=A^T b$.\n",
    "\n",
    "We find the same result as with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_distr(M):\n",
    "    A = np.append(np.matrix.transpose(M) - np.identity(len(M)), [np.ones(len(M))], axis=0)\n",
    "    A_T = np.matrix.transpose(A)\n",
    "    b = np.matrix.transpose( np.append( [ np.zeros( len(M)) ], [1] ) )\n",
    "    x = np.linalg.solve( A_T.dot(A), A_T.dot(b) )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stationary distribution is \n",
      " [ 0.31034483  0.24137931  0.44827586]\n"
     ]
    }
   ],
   "source": [
    "print(\"the stationary distribution is \\n\" , stat_distr(P)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Numerical Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function binomial logit using a normal distribution with $\\mu=.5$ and $\\sigma^2=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import stats\n",
    "def binomiallogit(b, pdf=sp.stats.norm.pdf):\n",
    "    x=.5\n",
    "    mu=.5\n",
    "    sigma=np.sqrt(2)\n",
    "    return (np.exp(b*x)/(1+np.exp(b*x)))*pdf(b, mu, sigma)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to integrate over $(-\\infty, \\infty)$ we get an error, the furthes we can go with the integration limits at around $(-1000, 1000)$ so we take this value as the true value of the integral. The outcome of this computation is called **Quad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value of the integral is 0.555939162843465\n"
     ]
    }
   ],
   "source": [
    "Quad, err=sp.integrate.quad(binomiallogit, -1000, 1000, epsrel=10**(-14))\n",
    "print('the true value of the integral is', Quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo\n",
    "So we have our baseline and we can now do the other methods and compare to the value that the built in function gives. First we do the Monte Carlo integration. To do so we define a function called **MC** which draws k values of $\\beta$ and evaluates the function $\\frac{\\exp(\\beta x)}{1+\\exp(\\beta x)}$ at the values of $\\beta$ that we draw and then we take the average of the value at all such draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Monte Carlo Approximation using 20 draws is 0.5706068398079811\n",
      "The Monte Carlo Approximation using 400 draws is 0.5480468227239329\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "def MC(k):\n",
    "    mu=.5\n",
    "    sigma=np.sqrt(2)\n",
    "    B=np.random.normal(mu, sigma, k)\n",
    "    x=.5\n",
    "    return np.mean([np.exp(b*x)/(1+np.exp(b*x)) for b in B])\n",
    "\n",
    "\n",
    "\n",
    "print('The Monte Carlo Approximation using 20 draws is', MC(20))\n",
    "print('The Monte Carlo Approximation using 400 draws is', MC(400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Hermite\n",
    "The other type of approximation that we do is using the Gauss-Hermite quadrature. We obtain the points and weights from the nuppy function and then take the weighted averge of the modified function. The function is called **GH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gauss-Hermite approximation with 4 points is 0.5559156754781621\n",
      "The Gauss-Hermite approximation with 12 points is 0.5559391624283003\n"
     ]
    }
   ],
   "source": [
    "def fun(t):\n",
    "    x=.5\n",
    "    return np.exp(t*x)/(1+np.exp(t*x))\n",
    "\n",
    "def GH(k):\n",
    "    mu=.5\n",
    "    sigma=np.sqrt(2)\n",
    "    pts, weigh= np.polynomial.hermite.hermgauss(k)\n",
    "    return (1/np.sqrt(np.pi))*np.sum(weigh.dot([fun(np.sqrt(2)*sigma*t+mu) for t in pts]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('The Gauss-Hermite approximation with 4 points is', GH(4))\n",
    "print('The Gauss-Hermite approximation with 12 points is', GH(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make sure that the weights are adding up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts, weigh= np.polynomial.hermite.hermgauss(4)\n",
    "np.sum(weigh/np.sqrt(np.pi))==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights sum up to one except for some odd values of points that we tried. Probably due to some rounding error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Approaches\n",
    "Finally we can compare all the approximations to the \"true\" value from the built in function. To do so we take the difference of each approximation with Quad (Quad-aporox):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value is 0.555939162843465\n",
      "The Monte Carlo Approximation with 20 draws gives 0.6182457278488706 so it is off by -0.015992118473345696\n",
      "The Monte Carlo Approximation with 400 draws gives 0.5461812662443476 so it is off by 0.00011001237488328375\n",
      "The Gauss-Hermite Approximation with 4 points gives 0.5559156754781621 so it is off by 2.3487365302887753e-05\n",
      "The Gauss-Hermite Approximation with 12 points gives 0.5559391624283003 so it is off by 4.151646804118059e-10\n"
     ]
    }
   ],
   "source": [
    "print('the true value is', Quad)\n",
    "print('The Monte Carlo Approximation with 20 draws gives', MC(20), 'so it is off by', Quad-MC(20))\n",
    "print('The Monte Carlo Approximation with 400 draws gives', MC(400), 'so it is off by', Quad-MC(400))\n",
    "print('The Gauss-Hermite Approximation with 4 points gives', GH(4), 'so it is off by', Quad-GH(4))\n",
    "print('The Gauss-Hermite Approximation with 12 points gives', GH(12), 'so it is off by', Quad-GH(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the best approximation is given by the Gauss-Hermite with 12 points with an error of the order of $e^{(-10)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Repeat with two dimensions\n",
    "First we do the dblquad integration function from python and call it **quad2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline value for the two dimension integral is 0.7296119616316019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def binomiallogit2(b1, b2, pdf=sp.stats.multivariate_normal.pdf):\n",
    "    x=np.array([.5, 1])\n",
    "    mu=np.array([.5, 1])\n",
    "    sigma=np.array([[np.sqrt(2), 0], [0,1]])\n",
    "    return (np.exp(b1*x[0]+b2*x[1])/(1+np.exp(b1*x[0]+b2*x[1])))*pdf([b1, b2], mu, sigma)\n",
    "    \n",
    "\n",
    "quad2, err2 = sp.integrate.dblquad(binomiallogit2, -300, 300, lambda i: -300, lambda i: 300, epsabs=10**(-14))\n",
    "print('the baseline value for the two dimension integral is', quad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo \n",
    "Now we draw betas from two different distributions. $N(.5, \\sqrt{2})$ as before and $N(1, 1)$. Now we have $x=[.5, 1]$. We evaluate the function at these pointa and take the simple average to get the Monte Carlo approximation. We call it **MC2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample means for the 20 and 400 Monte Carlo draws in the 2-D case are:\n",
      "20 draws:  0.7617100825344413\n",
      "400 draws:  0.7315479639363486\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def MC2(k):\n",
    "    x = np.array([.5, 1])\n",
    "    mu=np.array([.5, 1])\n",
    "    sigma=np.array([[np.sqrt(2), 0], [0,1]])\n",
    "    B = np.random.multivariate_normal(mu, sigma, k)\n",
    "    \n",
    "    return np.mean([(np.exp(np.inner(t,x))/(1+np.exp(np.inner(t,x)))) for t in B])\n",
    "\n",
    "\n",
    "\n",
    "print(\"The sample means for the 20 and 400 Monte Carlo draws in the 2-D case are:\")\n",
    "print(\"20 draws: \", MC2(20))\n",
    "print(\"400 draws: \", MC2(400))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Hermite \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
