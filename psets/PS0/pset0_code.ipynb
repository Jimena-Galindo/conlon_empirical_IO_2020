{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical IO PhD Class\n",
    "## Problem Set 0\n",
    "Jimena, Eyal and Pietro \n",
    "\n",
    "September 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Logit Function\n",
    "** 1. The log-sum-exp function is convex everywhere: **\n",
    "\n",
    "Pick any distinct $x, y \\in \\mathbb{R}^{N+1}$ and any $\\alpha \\in (0,1)$. \n",
    "$$f ( \\alpha x + ( 1 - \\alpha ) y ) = \\log \\sum_{i=0}^N \\exp(\\alpha x_i + ( 1 - \\alpha ) y_i)$$\n",
    "Applying Hölder's inequality to $\\sum_{i=0}^N \\exp(\\alpha x_i) \\exp( ( 1 - \\alpha ) y_i)$ with exponents $\\frac{1}{\\alpha}$ and $\\frac{1}{1-\\alpha}$ we get \n",
    "$$ \\sum_{i=0}^N \\exp(\\alpha x_i) \\exp( ( 1 - \\alpha ) y_i ) \\leq \\left [ \\sum_{i=0}^N |\\exp(\\alpha x_i)|^{\\frac{1}{\\alpha}}\\right ]^\\alpha \\left [ \\sum_{i=0}^N |\\exp((1 - \\alpha) y_i)|^{\\frac{1}{1 - \\alpha}} \\right ]^{1 - \\alpha}$$\n",
    "Taking logs on both sides and rearranging: \n",
    "$$ \\log \\sum_{i=0}^N exp( \\alpha x_i + ( 1 - \\alpha ) y_i) \\leq \\alpha \\log \\sum_{i=0}^N exp( x_i ) + ( 1 - \\alpha )\\log \\sum_{i=0}^N exp( y_i )  $$\n",
    "So the function is convex everywhere. \n",
    "\n",
    "** 2. Using the max trick: **\n",
    "\n",
    "Fix some $x \\in \\mathbb{R}^{N+1}$ and let $m:=\\max_i x_i$. Assume wlog $x_0=m$. \n",
    "We have \n",
    "$$ IV = \\log \\sum_{i=0}^N exp(x_i) = \\log \\sum_{i=0}^N exp(x_i) \\frac{exp(m)}{exp(m)}  $$\n",
    "and rearranging \n",
    "$$ IV = m + \\log ( 1 + \\sum_{i=1}^N \\exp ( x_i-m ) ) $$\n",
    "We have rescaled everything relative to the $\\max$ and added a constant. With this we take the exponential of smaller numbers and avoid the overflow problem. \n",
    "\n",
    "** 3. Comparing it to scipy.misc.logsumexp. Does it appear to suffer from underflow/overflow?\n",
    "Does it use the max trick?**\n",
    "\n",
    "First generate a tuple of values which includes some $x_i>600$ and evaluate the function at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "x=np.arange(10, 800, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the original IV equation we get the error (the evaluated number is infinity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pietro\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_1=np.log(np.sum(np.exp(x)))\n",
    "IV_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the *max* trick and compare to the value that we get from the logsumexp function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790.69316988129776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=max(x)\n",
    "IV=m+np.log(1+np.sum(np.exp(x-m)))\n",
    "IV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790.0000454009604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logsumexp and the function we modified have similar results but not exactly the same so logsumexp is doing something else to compute IV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### Stationary Distribution from eigenvectors\n",
    "Write a function that computes the ergodic distribution of the matrix\n",
    "$P$ by examining the properly rescaled eigenvectors and compare your result to $P^{100}$.\n",
    "\n",
    "We first define the transition matrix P and take the 100th power of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the transition matrix P \n",
      " [[ 0.2  0.4  0.4]\n",
      " [ 0.1  0.3  0.6]\n",
      " [ 0.5  0.1  0.4]]\n",
      "and this is the 100th power \n",
      " [[ 0.31034483  0.24137931  0.44827586]\n",
      " [ 0.31034483  0.24137931  0.44827586]\n",
      " [ 0.31034483  0.24137931  0.44827586]]\n",
      "\n",
      " this is the stationary using Quantecon package [[ 0.31034483  0.24137931  0.44827586]]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0.2, 0.4, 0.4],[0.1, 0.3, 0.6],[0.5, 0.1, 0.4]])\n",
    "print(\"This is the transition matrix P \\n\", P)\n",
    "P_100 = np.linalg.matrix_power(P, 100)\n",
    "print(\"and this is the 100th power \\n\", P_100)\n",
    "\n",
    "import quantecon as qe\n",
    "stat= qe.markov.core.mc_compute_stationary(P)\n",
    "print(\"\\n this is the stationary using Quantecon package\", stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iteration approach and the almighty John Stachurski agree on the stationary distribution. Let us try to compute it using the eigenvector method too.\n",
    "To calculate the stationary distribution we compute the eigenvalues and left eigenvectors of P. One of the eigenvalues is equal to one as P is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the eigenvalues are \n",
      " [ 1.00+0.j         -0.05+0.23979158j -0.05-0.23979158j]\n"
     ]
    }
   ],
   "source": [
    "P_T = np.matrix.transpose(P)\n",
    "w,v = np.linalg.eig(P_T)\n",
    "print(\"the eigenvalues are \\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first eigenvalue is equal to one, so the eigenvector associated to it will be our stationary distribution. Notice that the distribution needs to be a row vector so we need to transpose the eigenvectors we found. Indeed the stationary distribution $\\Pi$ satisfies $\\Pi' = P'\\Pi'$. I.e. it is the transpose of the right eigenvector associated with the unit eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvector we consider is \n",
      " [-0.52048344 -0.40482045 -0.75180941]\n"
     ]
    }
   ],
   "source": [
    "eigenvector = np.real( v.transpose()[0])\n",
    "print(\"The eigenvector we consider is \\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to rescale it so that the elements sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ergodic distribution found using eigenvectors is \n",
      " [ 0.31034483  0.24137931  0.44827586]\n"
     ]
    }
   ],
   "source": [
    "normalization = np.sum(eigenvector)\n",
    "Π = eigenvector / normalization \n",
    "print(\"The ergodic distribution found using eigenvectors is \\n\",Π)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare with what we found using matrix power, we find the two values are essentially identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of absolute discrepancies between the two matrices is \n",
      " 1.26842980563e-14\n"
     ]
    }
   ],
   "source": [
    "diff = np.sum ( np.abs(Π -P_100) )\n",
    "print(\"The sum of absolute discrepancies between the two matrices is \\n\",diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationaty Distribution from system of equations\n",
    "Just to show off our mathematical prowess, this section solves for the stationary distribution in a different way, solving the system of equations that define it.\n",
    "\n",
    "We want to have $\\pi$ such that $ (P^T-I)\\pi=0 $ and $\\sum_i \\pi_i=1$. We can write this as a system of equations $A\\pi=b$ with $A^T=[P^T-I, \\mathbb{1}]$ and $b=[\\mathbb{0}, 1]^T$ so that we can solve for $\\pi$ in $A^T A \\pi=A^T b$.\n",
    "\n",
    "We find the same result as with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_distr(M):\n",
    "    A = np.append(np.matrix.transpose(M) - np.identity(len(M)), [np.ones(len(M))], axis=0)\n",
    "    A_T = np.matrix.transpose(A)\n",
    "    b = np.matrix.transpose( np.append( [ np.zeros( len(M)) ], [1] ) )\n",
    "    x = np.linalg.solve( A_T.dot(A), A_T.dot(b) )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stationary distribution is \n",
      " [ 0.31034483  0.24137931  0.44827586]\n"
     ]
    }
   ],
   "source": [
    "print(\"the stationary distribution is \\n\" , stat_distr(P)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Numerical Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function binomial logit using a normal distribution with $\\mu=.5$ and $\\sigma^2=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import stats\n",
    "def binomiallogit(b, pdf=sp.stats.norm.pdf):\n",
    "    x=.5\n",
    "    return (np.exp(b*x)/(1+np.exp(b*x)))*pdf(b, 0.5, np.sqrt(2))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to integrate over $(-\\infty, \\infty)$ we get an error, the furthes we can go with the integration limits at around $(-1000, 100)$ so we take this value as the true value of the integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quad, err=sp.integrate.quad(binomiallogit, -1000, 1000, epsrel=10**(-14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=.5\n",
    "sigma=np.sqrt(2)\n",
    "b1=np.random.normal(mu, sigma, 20)\n",
    "b2=np.random.normal(mu, sigma, 400)\n",
    "\n",
    "def fun(t):\n",
    "    x=.5\n",
    "    return np.exp(t*x)/(1+np.exp(t*x))\n",
    "\n",
    "MC20=np.mean([fun(t) for t in b1])\n",
    "MC400=np.mean([fun(t) for t in b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5143105794690701\n",
      "0.5623660728833835\n"
     ]
    }
   ],
   "source": [
    "print(MC20)\n",
    "print(MC400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH(k):\n",
    "    pts, weigh= np.polynomial.hermite.hermgauss(k)\n",
    "    return (1/np.sqrt(np.pi))*np.sum(weigh.dot([fun(np.sqrt(2)*sigma*t+mu) for t in pts]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5559391624283003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GH(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts, weigh= np.polynomial.hermite.hermgauss(1)\n",
    "np.sum(weigh/np.sqrt(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights sum up to one. GH with 12 points is the closest to the true value, whereas Monte Carlo with 20 draws is the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.555939162843465\n",
      "0.5143105794690701\n",
      "0.5623660728833835\n",
      "0.5559156754781621\n",
      "0.5559391624283003\n"
     ]
    }
   ],
   "source": [
    "print(Quad)\n",
    "print(MC20)\n",
    "print(MC400)\n",
    "print(GH(4))\n",
    "print(GH(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomiallogit2(b1, b2, pdf=sp.stats.multivariate_normal.pdf):\n",
    "    x=np.array([.5, 1])\n",
    "    mu=np.array([.5, 1])\n",
    "    sigma=np.array([[np.sqrt(2), 0], [0,1]])\n",
    "    return (np.exp(b1*x[0]+b2*x[1])/(1+np.exp(b1*x[0]+b2*x[1])))*pdf([b1, b2], mu, sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07430684468717029"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "sp.stats.multivariate_normal.pdf(np.array([0, 0]), np.array([.5, 1]), np.array([[np.sqrt(2), 0], [0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037153422343585145"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binomiallogit2(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad2, err2 = sp.integrate.dblquad(binomiallogit2, -1, 1, lambda i: 0, lambda i: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09678615380415492"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
